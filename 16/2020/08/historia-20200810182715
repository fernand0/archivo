La tecnología, con cuidado, pero sin dejar de usarla
<a href="https://www.flickr.com/photos/fernand0/451304073/" title="'Jefe' con pinganillo "><img src="//live.staticflickr.com/250/451304073_fc360a41d2.jpg" alt="'Jefe' con pinganillo " width="240" style="float:left; margin:5px"></a>
Hemos pasado unos meses intensos. Por unas semanas, incluso hemos visto como mucha gente veía la parte positiva de la tecnología (que en nuestro contexto, es bastante peculiar porque es muy habitual hablar de lo mala que es, y de los riesgos y peligro que tiene).
Sin embargo, no podemos decir aquello de 'prueba superada'. Conforme nuestra realidad se van 'normalizando' (ya podemos salir a la calle e ir a la oficina, el peligro no ha pasado), volvemos a escuchar los discursos anti-tecnología (y no sólo por los 'adalides' del anti-todismo, sino por mucha gente y medios de información 'normales'.
Esto nos deja donde estamos, claro: dependientes completamente de la tecnología que desarrollan otros porque en nuestro 'caldo de cultivo' muy difícilmente se desarrollará nada que vaya a ser muy rompedor.

En pocos días vamos a ver si el Gobierno (y las regiones) se decide a apoyar la aplicación de seguimiento de contactos y veremos campañas diciendo que es muy invasiva, que tenemos que tener cuidado y recomendaciones de no instalarla. 

Este preámbulo tan largo me sirve para hablar de un análisis bastante negativo de una tecnología, el aprendizaje automático, que el autor trata de ayudarnos a comprender mejor para que tengamos cuidado cuando (sin duda) la usemos.

En <a href="Our Neophobic, Conservative AI Overlords Want Everything to Stay the Same">Our Neophobic, Conservative AI Overlords Want Everything to Stay the Same</a> Cory Doctorow nos recuerda que esta actitud antitecnológica debe manejarse con cuidado, y deben tenerse en cuenta los posibles defectos y problemas. Pero eso significa que debemos usarla con cuidado y  tratar de hacer y avanzar). 

En este caso, habla de la inteligencia artificial y las decisiones que se tomarán con ella. Recordemos que lo que sirve para que un sistema de inteligencia artificial aprenda es los datos que ya tenemos; esto es, las decisiones ya tomadas, los sesgos que ya existían,...
 
<blockquote>
... machine learning is fundamentally conservative, and it hates change. If you start a text message to your partner with “Hey darling,” the next time you start typing a message to them, “Hey” will beget an autosuggestion of “darling” as the next word, even if this time you are announcing a break-up. If you type a word or phrase you’ve never typed before, autosuggest will prompt you with the statistically most common next phrase from all users (I made a small internet storm in July 2018 when I documented autocomplete’s suggestion in my message to the family babysitter, which paired “Can you sit” with “on my face and”).
</blockquote>

Es lo mismo que sucede con los anuncios que nos 'persiguen' en internet: cuando buscamos algo, los sistemas nos siguen recomendando versiones de ese algo para ver si consiguen que lo compremos (y además no siempre 'saben' si lo hemos comprado o no):

<blockquote>
This conservativeness permeates every system of algorithmic inference: search for a refrigerator or a pair of shoes and they will follow you around the web as machine learning systems “re-target” you while you move from place to place, even after you’ve bought the fridge or the shoes. 
</blockquote>

Porque al final, el aprendizaje automático (<em>machine learning</em>) es una herramienta muy buena para encontrar similitudes con el modelo que la herramienta tenga disponible; esto es, para darnos 'más de lo mismo'.

<blockquote>
Ultimately, machine learning is about finding things that are similar to things the machine learning system can already model. Machine learning systems are good at identifying cars that are similar to the cars they already know about
</blockquote>

Y por eso, a veces, esto es un problema: un sistema que decida a quién hay que detener, nos propondrá detener a personas parecidas a las que ya hay detenidas.

<blockquote>
f you ask an ML system to predict who the police should arrest, it will suggest that they go and arrest people similar to the ones they’ve been arresting all along. 
</blockquote>

Porque, en realidad, no predicen el crime, sino cómo se gestiona

<blockquote>
As the Human Rights Data Analysis Group’s Patrick Ball puts it, “A predictive policing system doesn’t predict crime, it predicts policing.”
</blockquote>

Y, como decíamos arriba, esto no significa que debamos abandonar una tecnología (o ni siquiera intentemos usarla), sino que lo que hay que hacer es tener cuidado en cómo se usa, quién la usa, y para qué.

<blockquote>
Data analysis is as old as censuses of the tax collectors of antiquity — it’s as old as the Book of Numbers! — and it is unquestionably useful. 
...
The question of what the technology does is important, but far more important is who it is doing it for and who it is doing it to.
</blockquote>

Que, seguramente, es en lo que se diferenciaría un analista medio en nuestro contexto de Cory Doctorow en, por ejemplo, este texto.

 Dónde:Reflexiones e Irreflexiones
 URL:http://fernand0.blogalia.com//historias/None
